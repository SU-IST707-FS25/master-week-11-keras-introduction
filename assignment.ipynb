{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9958f4c",
   "metadata": {
    "problem_id": "ex1"
   },
   "source": [
    "## Exercise 1 Understanding MLPs and Network Architecture\n",
    "\n",
    "Complete to code to accomplish the following:\n",
    "\n",
    "1. Train each architecture for 50 epochs\n",
    "2. Record training and validation accuracy for each\n",
    "3. Plot the learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df92f06",
   "metadata": {
    "part_id": "ex1-part1",
    "span": "ex1-part1.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load and prepare data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_model(layer_sizes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_sizes[0], activation='relu', input_shape=(4,)))\n",
    "    for size in layer_sizes[1:-1]:\n",
    "        model.add(Dense(size, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Test different architectures\n",
    "architectures = [\n",
    "    [10, 3],\n",
    "    [20, 10, 3],\n",
    "    [30, 20, 10, 3]\n",
    "]\n",
    "\n",
    "# TODO: Replace with your code (fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62fbc0c",
   "metadata": {
    "part_id": "ex1-part2"
   },
   "source": [
    "How did your different architectures perform?  Why do you think that is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107c54f",
   "metadata": {
    "part_id": "ex1-part2",
    "span": "ex1-part2.answer",
    "student": true
   },
   "source": [
    "*Enter your answer in this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd5ac9",
   "metadata": {
    "problem_id": "ex2"
   },
   "source": [
    "## Exercise 2: Impact of Batch Size and Learning Rate\n",
    "\n",
    "\n",
    "Using the best architecture from Exercise 1, complete the following code to explore how batch size and learning rate affect training:\n",
    "\n",
    "Tasks:\n",
    "1. Create a grid of experiment plots testing the different batch sizes and learning rates\n",
    "2. Each plot should show training and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3db4af1",
   "metadata": {
    "part_id": "ex2-part1",
    "span": "ex2-part1.code",
    "student": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a8c675",
   "metadata": {
    "part_id": "ex2-part2"
   },
   "source": [
    "Analyze how the different parameters affected:\n",
    "   - Training speed\n",
    "   - Final accuracy\n",
    "   - Stability of training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1140ef0d",
   "metadata": {
    "part_id": "ex2-part2",
    "span": "ex2-part2.answer",
    "student": true
   },
   "source": [
    "*Enter your answer in this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed86fa",
   "metadata": {
    "problem_id": "ex3"
   },
   "source": [
    "## Exercise 3: Comparing MLPs with Traditional Models\n",
    "\n",
    "<!-- @q -->\n",
    "\n",
    "1. Complete the MLP comparison code; start with at least a couple of hidden layers. Use relu for the internal layers and adam as an optimizer.\n",
    "2. Run comparisons on both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c58959",
   "metadata": {
    "part_id": "ex3-part1",
    "span": "ex3-part1.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Load datasets\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X_c, y_c = cancer.data, cancer.target\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test_full, y_test_full) = fashion_mnist.load_data()\n",
    "X_f = X_train_full[:1000].reshape(1000, -1) / 255.0\n",
    "y_f = y_train_full[:1000]\n",
    "\n",
    "def compare_models(X, y, name=\"Dataset\"):\n",
    "    # Split and scale data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    rf_score = rf.score(X_test_scaled, y_test)\n",
    "    \n",
    "# TODO: Replace with your code (fill)\n",
    "\n",
    "    return rf_score, mlp_score\n",
    "\n",
    "# Compare on both datasets\n",
    "cancer_results = compare_models(X_c, y_c, \"Cancer Dataset\")\n",
    "fashion_results = compare_models(X_f, y_f, \"Fashion MNIST Subset\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b1945",
   "metadata": {
    "part_id": "ex3-part2"
   },
   "source": [
    "Try varying network architecture (number and width of layers), epochs, and learning rate. Use google to find out more about these two datasets. What do you observe? Does one data set seem \"easier\" for one of the two methods?  If so why?  If not why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d905b",
   "metadata": {
    "part_id": "ex3-part2",
    "span": "ex3-part2.answer",
    "student": true
   },
   "source": [
    "*Enter your answer in this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a765e5",
   "metadata": {
    "problem_id": "ex4"
   },
   "source": [
    "## Exercise 4 Solution: Early Stopping and Overfitting\n",
    "<!-- @q -->\n",
    "\n",
    "In the following, complete the code to compare the impact of early stopping for a complex network.  Use the same model parameters (by calling `create_complex_model()`) for both tests.\n",
    "\n",
    "1. Train the model without early stopping for 400 epochs\n",
    "2. Implement early stopping with appropriate parameters\n",
    "3. Visualize training and validation curves \n",
    "4. Compare final test performance\n",
    "4. Visualize decision boundaries for both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11f8b7",
   "metadata": {
    "part_id": "ex4-part1",
    "span": "ex4-part1.fill",
    "student": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate dataset\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "def create_complex_model():\n",
    "    model = Sequential([\n",
    "        Dense(100, activation='relu', input_shape=(2,)),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Plot decision boundaries\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                        np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.3, s=2)\n",
    "\n",
    "# TODO: Replace with your code (fill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6118c0",
   "metadata": {
    "part_id": "ex4-part2"
   },
   "source": [
    "What is the impact of early stopping on accuracy?  What do you learn from looking at the decision boundaries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df579e",
   "metadata": {
    "part_id": "ex4-part2",
    "span": "ex4-part2.answer",
    "student": true
   },
   "source": [
    "*Enter your answer in this cell*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd6957e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
